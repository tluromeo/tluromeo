import re
import math
import random

# SYLLABLE PARSING CODE

# opening text file and creating a list of all entries
f = open ("p-p.txt", "r") 
words = f.readlines()

# extracting first column of the text file (just the words)
for i in range(len(words)): 
    words[i] = re.split ('  ', words[i])[0]

# extracting the onset of each word, if any (parsing at first vowel)
onsets = []
for i in range (len(words)): 
    onsets.append(re.split('a|e|i|o|u', words[i])[0])

# creating a dictionary of all the onsets we extracted (format: onset, frequency)
onset_dict = {}
for o in onsets:
    if o in onset_dict: onset_dict[o] += 1
    else: onset_dict[o] = 1
del onset_dict['']

# sorting onset dictionary by frequency 
onset_dict = dict (sorted(onset_dict.items(), key=lambda x: x[1], reverse=True))

# deleting all onsets that occur less than 10 times (MAYBE CHANGE THIS)
valid_onsets = {key:value for (key,value) in onset_dict.items() if value >= 10}

# based on the fact that y is only a consonant at the start of a word or syllable (MAYBE CHANGE THIS), 
# removing all the onsets that contain a y which is not at the start 
# https://www.woodwardenglish.com/letter-y-vowel-or-consonant/
valid_onsets = {key:value for (key,value) in valid_onsets.items() if 'y' not in key or key[0] == 'y'}

# we now have a dictionary of 65 valid onsets, ranked by frequency !!

# function which accepts a word and outputs a list of its distinct syllables
def syllables (word): 
    vowels = ['a', 'e', 'i', 'o', 'u']
    
    # letter pairs that are not valid onsets by themselves, but are part of a three-consonant valid onset
    exceptions = ['hr', 'hl'] 
    
    # default initial values
    sylls = []
    check_onset = False
    syllable = ""
    onset = ""

    # looping through the word, end to beginning, using an accumulator to keep track of the letters 
    # we've checked. when a vowel is reached, the variable check_onset is set to true, at which 
    # point the letters leading up to the vowel are used to create the longest onset possible. 
    # once this onset is formed, the onset + vowel + consonants acculumated after the vowel are 
    # concatonated to form a syllable, which is stored in the sylls list. once the syllable is formed 
    # and stored, check_onset is reset to false and the process of creating another syllable begins anew. 
    for i in range (len(word) - 1, -1, -1): 

        # 1. occurs when we are NOT interested in whether the current letter is part of a valid onset
         if not check_onset:
            
            # add the current letter to our syllable accumulator
            syllable = word[i] + syllable
            
            # if we've made it to the first letter of the word and we don't care about whether
            # it's part of an onset (it's probably a vowel), then we just store the entire
            # syllable we've accumulated thus far
            if i == 0: 
                sylls.insert(0, syllable)
            
            # if we encounter an "ing" in our syllable accumulator, and we note that the letters 
            # preceding this syllable form a valid word in our database, we store "ing" as its
            # own syllable and restart the syllable accumulating process
            elif syllable == "ing" and len (word[:i]) >=2 and word[:i] in words: 
                sylls.insert (0, syllable)
                syllable = ""
           
            # if we've hit a vowel that is not preceded by another vowel (i.e. either a lone
            # vowel or the first vowel in a series of vowels), we trigger the onset check (CHANGE THIS)
            elif word[i] in vowels and word [i - 1] not in vowels: 
                check_onset = True

                # if a word ends in an e, we assume that the e
                # is silent and therefore do not trigger an onset check (CHANGE THIS)
                if i == len (word) - 1 and word[i] == 'e' : 
                    check_onset = False
            
            # if the last letter of the word is y and the second to last letter is not a vowel, 
            # we treat y as a vowel and trigger an onset check (MAYBE CHANGE THIS)
            elif i == len(word) - 1 and word[i] == 'y' and word [i-1] not in vowels: 
                check_onset = True

         # 2. occurs when we ARE interested in whether the current letter is part of a valid onset
         else: 

            # add the current letter to our onset accumulator
            onset = word[i] + onset

            # 2a. if we've made it to the first letter of the word, we have to check to see
            # whether the onset we've accumulated is a valid onset or not. if it is, we can 
            # add it in front of the vowel + other consonants we've accumulated to make a syllable.
            # if it is not, then we assume the letter is its own syllable and add two entries to
            # our syllable list â€” the letter, and the fragment of the onset that was valid + the vowel
            # + the rest of the consonants
            if i == 0:
                if not onset in valid_onsets: 
                    sylls.insert(0, onset[1 : len(onset)] + syllable)
                    sylls.insert(0, word[i])
                else: 
                    sylls.insert (0, onset + syllable)

            # 2b. if the onset we've accumulated thus far is not valid (and isn't a valid onset fragment
            # stored in exceptions), then we first add the valid onset fragment + vowel + other 
            # accumulated consonants to our syllable list, and reset our syllable accumulator
            # to include just the letter we rejected as part of the valid onset. we also reset
            # the onset accumulator since we've established that the current letter is not 
            # part of an onset. finally, we determine whether or not we're supposed to continue
            # building our current onset, and store this answer in check_onset
            elif not onset in valid_onsets and (not onset in exceptions):
                sylls.insert(0, onset[1 : len(onset)] + syllable)
                syllable = word[i]
                onset = ""
                
                # if we have an e at the end our new syllable, we assume it is silent and 
                # do not start checking for onsets (CHANGE THIS)
                
                # if word[i] == 'e': 
                #     check_onset = False

                # if we have a single vowel at the end of our new syllable, we start
                # checking for onsets (CHANGE THIS)
                if word[i] in vowels and word [i - 1] not in vowels: 
                    check_onset = True
                
                # if we have a y that is not preceded by a vowel at the end of our new
                # syllable, we start checking for onsets
                elif word[i] == 'y' and word [i - 1] not in vowels: 
                    check_onset = True
                
                # by default, we assume we're not checking for onsets because we just
                # starting accumulating a new syllable
                else: 
                    check_onset = False

            # note that if neither 2a nor 2b is triggered, all we have done is add the
            # current letter to our onset accumulator and assume that we should continue
            # searching for a valid onset before we can create a new syllable
                
    return (sylls)

# printing out our results (word followed by syllabification)
with open ('syllables1.txt', 'w') as f: 
    for w in words: 
        f.write(w + ": " + str(syllables(w)))
        f.write ('\n')


# PREPPING LIST OF MONOSYLLABIC WORDS TO RUN TOLERANCE PRINCIPLE ON
vowels = ['a', 'e', 'i', 'o', 'u']

f = open ("p-p.txt", "r") 
full_words = f.readlines()

mono_words = [] 

# extracting words and pronunciations from the text file
for i in range(len(words)): 
    full_words[i] = [re.split ('  ', full_words[i])[0], re.split ('  ', full_words[i])[1][0:-1]]
    if len(syllables(full_words[i][0])) == 1:
        mono_words.append(full_words[i])

# cleaning up our list of sample words 
mono_words.remove(['little', 'l ih1 t ah0 l'])
mono_words.remove(['mr', 'm ih1 s t er0'])
mono_words.remove(['people', 'p iy1 p ah0 l'])
mono_words.remove(['goodbye', 'g uh2 d b ay1'])
mono_words.remove(['handle', 'hh ae1 n d ah0 l'])
mono_words.remove(['tv', 't iy1 v iy1'])
mono_words.remove(['joey', 'jh ow1 iy0'])
mono_words.remove(['ms', 'm ih1 z'])
mono_words.remove(['maybe', 'm ey1 b iy0'])
mono_words.remove(['dr', 'd r ay1 v'])
mono_words.remove(['mrs', 'm ih1 s ih0 z'])
mono_words.remove(['trouble', 't r ah1 b ah0 l'])
mono_words.remove(['couple', 'k ah1 p ah0 l'])
mono_words.remove(['able', 'ey1 b ah0 l'])
mono_words.remove(['ls', 'eh1 l eh1 s'])
mono_words.remove(['simple', 's ih1 m p ah0 l'])
mono_words.remove(['middle', 'm ih1 d ah0 l'])
mono_words.remove(['ln', 'l ey1 n'])
mono_words.remove(['single', 's ih1 ng g ah0 l'])
mono_words.remove(['double', 'd ah1 b ah0 l'])
mono_words.remove(['client', 'k l ay1 ah0 n t'])
mono_words.remove(['bottle', 'b aa1 t ah0 l'])
mono_words.remove(['st', 's t r iy1 t'])
mono_words.remove(['battle', 'b ae1 t ah0 l'])
mono_words.remove(['settle', 's eh1 t ah0 l'])
mono_words.remove(['science', 's ay1 ah0 n s'])
mono_words.remove(['brian', 'b r ay1 ah0 n'])
mono_words.remove(['ruin', 'r uw1 ah0 n'])
mono_words.remove(['louis', 'l uw1 ih0 s'])
mono_words.remove(['hire', 'hh ay1 er0'])
mono_words.remove(['jesse', 'jh eh1 s iy0'])
mono_words.remove(['create', 'k r iy0 ey1 t'])
mono_words.remove(['clients', 'k l ay1 ah0 n t s'])
mono_words.remove(['se', 's aw2 th iy1 s t'])
mono_words.remove(['bible', 'b ay1 b ah0 l'])
mono_words.remove(['temple', 't eh1 m p ah0 l'])
mono_words.remove(['daphne', 'd ae1 f n iy0'])
mono_words.remove(['turtle', 't er1 t ah0 l'])
mono_words.remove(['gentle', 'jh eh1 n t ah0 l'])
mono_words.remove(['whistle', 'w ih1 s ah0 l'])
mono_words.remove(['sample', 's ae1 m p ah0 l'])
mono_words.remove(['noble', 'n ow1 b ah0 l'])
mono_words.remove(['rifle', 'r ay1 f ah0 l'])
mono_words.remove(['diane', 'd ay0 ae1 n'])
mono_words.remove(['brooklyn', 'b r uh1 k l ah0 n'])
mono_words.remove(['poem', 'p ow1 ah0 m'])
mono_words.remove(['muscle', 'm ah1 s ah0 l'])
mono_words.remove(['struggle', 's t r ah1 g ah0 l'])
mono_words.remove(['cattle', 'k ae1 t ah0 l'])
mono_words.remove(['stable', 's t ey1 b ah0 l'])
mono_words.remove(['stuart', 's t uw1 er0 t'])
mono_words.remove(['purple', 'p er1 p ah0 l'])
mono_words.remove(['theatre', 'th iy1 ah0 t er0'])
mono_words.remove(['rhythm', 'r ih1 dh ah0 m'])
mono_words.remove(['centre', 's eh1 n t er0'])
mono_words.remove(['humble', 'hh ah1 m b ah0 l'])
mono_words.remove(['louie', 'l uw0 iy1'])
mono_words.remove(['poet', 'p ow1 ah0 t'])
mono_words.remove(['gamble', 'g ae1 m b ah0 l'])
mono_words.remove(['fluid', 'f l uw1 ah0 d'])
mono_words.remove(['noah', 'n ow1 ah0'])
mono_words.remove(['prior', 'p r ay1 er0'])
mono_words.remove(['candle', 'k ae1 n d ah0 l'])
mono_words.remove(['ankle', 'ae1 ng k ah0 l'])
mono_words.remove(['bubble', 'b ah1 b ah0 l'])
mono_words.remove(['hustle', 'hh ah1 s ah0 l'])
mono_words.remove(['saddle', 's ae1 d ah0 l'])
mono_words.remove(['aisle', 'ay1 l'])
mono_words.remove(['puzzle', 'p ah1 z ah0 l'])
mono_words.remove(['pierre', 'p iy0 eh1 r'])
mono_words.remove(['react', 'r iy0 ae1 k t'])
mono_words.remove(['subtle', 's ah1 t ah0 l'])
mono_words.remove(['rio', 'r iy1 ow0'])
mono_words.remove(['bicycle', 'b ay1 s ih0 k ah0 l'])
mono_words.remove(['andre', 'aa1 n d r ey2'])
mono_words.remove(['est', 'ah0 s t ey1 t'])
mono_words.remove(['cycle', 's ay1 k ah0 l'])
mono_words.remove(['shuttle', 'sh ah1 t ah0 l'])
mono_words.remove(['monte', 'm aa1 n t iy0'])
mono_words.remove(['egypt', 'iy1 jh ah0 p t'])
mono_words.remove(['ct', 'k ao1 r t'])
mono_words.remove(['marble', 'm aa1 r b ah0 l'])
mono_words.remove(['dante', 'd aa1 n t ey0'])
mono_words.remove(['jingle', 'jh ih1 ng g ah0 l'])
mono_words.remove(['buckle', 'b ah1 k ah0 l'])
mono_words.remove(['sr', 's iy1 n y er0'])
mono_words.remove(['tickle', 't ih1 k ah0 l'])
mono_words.remove(['triumph', 't r ay1 ah0 m f'])
mono_words.remove(['pickle', 'p ih1 k ah0 l'])
mono_words.remove(['riddle', 'r ih1 d ah0 l'])
mono_words.remove(['strangle', 's t r ae1 ng g ah0 l'])
mono_words.remove(['giants', 'jh ay1 ah0 n t s'])
mono_words.remove(['posse', 'p aa1 s iy0'])
mono_words.remove(['tackle', 't ae1 k ah0 l'])
mono_words.remove(['triangle', 't r ay1 ae2 ng g ah0 l'])
mono_words.remove(['padre', 'p ae1 d r ey2'])
mono_words.remove(['vs', 'v er1 s ah0 z'])
mono_words.remove(['fiance', 'f iy0 aa1 n s ey2'])
mono_words.remove(['trials', 't r ay1 ah0 l z'])
mono_words.remove(['cripple', 'k r ih1 p ah0 l'])
mono_words.remove(['paddle', 'p ae1 d ah0 l'])
mono_words.remove(['fiddle', 'f ih1 d ah0 l'])
mono_words.remove(['cc', 's iy1 s iy1'])
mono_words.remove(['ok', 'ow1 k ey1'])
mono_words.remove(['wrestle', 'r eh1 s ah0 l'])
mono_words.remove(['kyle', 'k ay1 l'])
mono_words.remove(['giant', 'jh ay1 ah0 n t'])
mono_words.remove(['apple', 'ae1 p ah0 l'])
mono_words.remove(['jungle', 'jh ah1 ng g ah0 l'])
mono_words.remove(['castle', 'k ae1 s ah0 l'])
mono_words.remove(['circle', 's er1 k ah0 l'])
mono_words.remove(['gimme', 'g ih1 m iy0'])
mono_words.remove(['noel', 'n ow0 eh1 l'])
mono_words.remove(['title', 't ay1 t ah0 l'])
mono_words.remove(['chloe', 'k l ow1 iy0'])
mono_words.remove(['angle', 'ae1 ng g ah0 l'])
mono_words.remove(['louise', 'l uw0 iy1 z'])
mono_words.remove(['seattle', 's iy0 ae1 t ah0 l'])
mono_words.remove(['needle', 'n iy1 d ah0 l'])
mono_words.remove(['liable', 'l ay1 ah0 b ah0 l'])
mono_words.remove(['riot', 'r ay1 ah0 t'])
mono_words.remove(['naive', 'n ay2 iy1 v'])
mono_words.remove(['pm', 'p iy1 eh1 m'])
mono_words.remove(['mia', 'm iy1 ah0'])
mono_words.remove(['liars', 'l ay1 er0 z'])
mono_words.remove(['via', 'v ay1 ah0'])
mono_words.remove(['rattle', 'r ae1 t ah0 l'])
mono_words.remove(['idle', 'ay1 d ah0 l'])
mono_words.remove(['beings', 'b iy1 ih0 ng z'])
mono_words.remove(['eagle', 'iy1 g ah0 l'])
mono_words.remove(['leon', 'l iy1 aa0 n'])
mono_words.remove(['triple', 't r ih1 p ah0 l'])
mono_words.remove(['cd', 's iy1 d iy1'])
mono_words.remove(['ph', 'p iy1 ey1 ch'])
mono_words.remove(['fluids', 'f l uw1 ah0 d z'])
mono_words.remove(['leo', 'l iy1 ow0'])
mono_words.remove(['liar', 'l ay1 er0'])
mono_words.remove(['maple', 'm ey1 p ah0 l'])
mono_words.remove(['phoebe', 'f iy1 b iy0'])
mono_words.remove(['th', 't iy1 ey1 ch'])
mono_words.remove(['myrtle', 'm er1 t ah0 l'])
mono_words.remove(['flour', 'f l aw1 er0'])
mono_words.remove(['table', 't ey1 b ah0 l'])
mono_words.remove(['twinkle', 't w ih1 ng k ah0 l'])
mono_words.remove(['toa', 't ow1 ah0'])
mono_words.remove(['squire', 's k w ay1 r'])
mono_words.remove(['wiggle', 'w ih1 g ah0 l'])
mono_words.remove(['shuffle', 'sh ah1 f ah0 l'])
mono_words.remove(['jekyll', 'jh eh1 k ah0 l'])
mono_words.remove(['noodle', 'n uw1 d ah0 l'])
mono_words.remove(['neon', 'n iy1 aa0 n'])
mono_words.remove(['poodle', 'p uw1 d ah0 l'])
mono_words.remove(['cradle', 'k r ey1 d ah0 l'])
mono_words.remove(['nipple', 'n ih1 p ah0 l'])
mono_words.remove(['kettle', 'k eh1 t ah0 l'])
mono_words.remove(['psyche', 's ay1 k iy0'])
mono_words.remove(['rumble', 'r ah1 m b ah0 l'])
mono_words.remove(['mingle', 'm ih1 ng g ah0 l'])
mono_words.remove(['etc', 'eh2 t s eh1 t er0 ah0'])
mono_words.remove(['hassle', 'hh ae1 s ah0 l'])
mono_words.remove(['martyr', 'm aa1 r t er0'])
mono_words.remove(['bundle', 'b ah1 n d ah0 l'])
mono_words.remove(['poets', 'p ow1 ah0 t s'])
mono_words.remove(['scramble', 's k r ae1 m b ah0 l'])
mono_words.remove(['latte', 'l aa1 t ey2'])
mono_words.remove(['viable', 'v ay1 ah0 b ah0 l'])
mono_words.remove(['gobble', 'g aa1 b ah0 l'])
mono_words.remove(['throttle', 'th r aa1 t ah0 l'])
mono_words.remove(['mc', 'm ih0 k'])
mono_words.remove(['tremble', 't r eh1 m b ah0 l'])
mono_words.remove(['huddle', 'hh ah1 d ah0 l'])
mono_words.remove(['ante', 'ae1 n t iy0'])
mono_words.remove(['neo', 'n iy1 ow0'])
mono_words.remove(['mantle', 'm ae1 n t ah0 l'])
mono_words.remove(['cuddle', 'k ah1 d ah0 l'])
mono_words.remove(['ac', 'ey1 s iy1'])
mono_words.remove(['waffle', 'w aa1 f ah0 l'])
mono_words.remove(['trifle', 't r ay1 f ah0 l'])
mono_words.remove(['doodle', 'd uw1 d ah0 l'])
mono_words.remove(['spoils', 's p oy1 l z'])
mono_words.remove(['beetle', 'b iy1 t ah0 l'])
mono_words.remove(['smuggle', 's m ah1 g ah0 l'])
mono_words.remove(['pc', 'p iy1 s iy1'])
mono_words.remove(['boils', 'b oy1 l z'])
mono_words.remove(['buick', 'b y uw1 ih0 k'])
mono_words.remove(['stumble', 's t ah1 m b ah0 l'])
mono_words.remove(['tumble', 't ah1 m b ah0 l'])
mono_words.remove(['notre', 'n ow1 t r ah0'])
mono_words.remove(['puddle', 'p ah1 d ah0 l'])
mono_words.remove(['mt', 'm aw1 n t'])
mono_words.remove(['wrinkle', 'r ih1 ng k ah0 l'])
mono_words.remove(['scrabble', 's k r ae1 b ah0 l'])
mono_words.remove(['raffle', 'r ae1 f ah0 l'])
mono_words.remove(['ample', 'ae1 m p ah0 l'])
mono_words.remove(['acre', 'ey1 k er0'])
mono_words.remove(['abyss', 'ah0 b ih1 s'])
mono_words.remove(['rubble', 'r ah1 b ah0 l'])
mono_words.remove(['friar', 'f r ay1 er0'])
mono_words.remove(['bugle', 'b y uw1 g ah0 l'])
mono_words.remove(['giggle', 'g ih1 g ah0 l'])
mono_words.remove(['ts', 't iy1 eh1 s'])
mono_words.remove(['feeble', 'f iy1 b ah0 l'])
mono_words.remove(['sparkle', 's p aa1 r k ah0 l'])
mono_words.remove(['pimple', 'p ih1 m p ah0 l'])
mono_words.remove(['triage', 't r ay1 ih0 jh'])
mono_words.remove(['hombre', 'hh aa1 m b r ah0'])
mono_words.remove(['bias', 'b ay1 ah0 s'])
mono_words.remove(['muzzle', 'm ah1 z ah0 l'])
mono_words.remove(['chuckle', 'ch ah1 k ah0 l'])
mono_words.remove(['luau', 'l uw1 aw2'])
mono_words.remove(['girdle', 'g er1 d ah0 l'])
mono_words.remove(['jose', 'hh ow2 z ey1'])
mono_words.remove(['lions', 'l ay1 ah0 n z'])

# removing all single letters from our list of monosyllabic words
for i in range (3500): 
    if len(mono_words[i][0]) == 1 and mono_words[i][0] not in vowels: 
        mono_words.remove(mono_words[i])

# as of right now, we are interested in the first 3500 monosyllabic words
mono_words = mono_words[0:3500]

# extracting list of monosyllabic words to use in testing set
test_words = []
test_correct = []

for i in range (500): 
    index = random.randint(0, len(mono_words)-1)
    test_words.append(mono_words[index][0])
    test_correct.append(mono_words[index][1])
    del mono_words[index]

# note: the rest of the words not in the testing set will remain in the training set
# let's print these out
with open ('monosyllable.txt', 'w') as f: 
    for i in range (len(mono_words)): 
        f.write (str(mono_words[i]) + "\n")

# program to process the monosyllabic words in the training set
def make_entry (entry): 

    # splitting the word at the first vowel (returns an array)
    split1 = re.split('(a|e|i|o|u)', entry[0])

    # onset and rime of the word
    onset = split1[0]
    
    # if the onset is equal to the entire word (no vowels, only y), 
    # then we don't want to accidentally include the onset code with the rime
    y_case = False 
    if onset == entry[0]: 
        y_case = True
    
    rime = ""
    for i in range (1, len(split1)): 
        rime += split1[i]
    
    # splitting the pronunciation code at spaces
    split2 = re.split (' ', entry[1])
    add_rime = False
    onset_code, rime_code = "", ""

    # building the onset code and rime code
    for i in range (len(split2)): 
        if split2[i][0] in vowels: 
            add_rime = True
        if add_rime and not y_case: 
            rime_code += str(split2[i]) + " "
        else: 
            onset_code += str(split2[i]) + " "
    
    # 4-part return: onset, onset code, rime, and rime code
    return [onset, onset_code[0:-1], rime, rime_code[0:-1]]

# creating a list of entries by inputting our training set words into make_entry
entries = []
for i in range(3000): 
    entries.append(make_entry(mono_words[i]))

all_onsets = {}
onset_dict = {}
all_rimes = {}
rime_dict = {}

for [onset, onset_code, rime, rime_code] in entries: 
    
    # building the list of all possible onsets
    if onset in all_onsets: all_onsets[onset] += 1
    else: all_onsets [onset] = 1

    # building the list of valid onset/code pairs
    if (onset, onset_code) in onset_dict: onset_dict[(onset, onset_code)] += 1
    else: onset_dict[onset, onset_code] = 1

    # building the list of all possible rimes
    if rime in all_rimes: all_rimes[rime] += 1
    else: all_rimes [rime] = 1

    # building the list of valid rime/code pairs
    if (rime, rime_code) in rime_dict: rime_dict[(rime, rime_code)] += 1
    else: rime_dict[(rime, rime_code)] = 1

del all_onsets['']

if ('', 'y') in onset_dict: 
    del onset_dict[('', 'y')]
if ('', 'w') in onset_dict: 
    del onset_dict[('', 'w')]

def read_onset (o): 
    if o in all_onsets: total = float(all_onsets[o])
    elif o == '': total = 0
    else: return ["onset not in dictionary â€” need to guess", 0]
    
    if total == 1: tp = 1
    elif total == 0: tp = 0
    else: tp = math.floor(total / math.log(total))

    for (onset, code) in onset_dict: 
        if o == onset:
            if (total - onset_dict[(onset, code)]) <= tp: 
                return [code, tp]
    return ["onset tolerance principle not met â€” need to memorize exceptions", tp]

def pronounce (word): 
    # print("currently pronouncing: " + word)
    split = re.split('(a|e|i|o|u)', word)
    rime_to_match = ""
    # print(rime_to_match)
    match_found = False

    for i in range (1, len(split)): 
        rime_to_match += split[i]
    
    onset_tp = read_onset(split[0])[1]

    if read_onset(split[0])[0] == '': 
        output = read_onset(split[0])[0]
    else: 
        output = read_onset(split[0])[0] + " "

    if rime_to_match in all_rimes: 
        total = float(all_rimes[rime_to_match])
    else: 
        # if word[-1] == "s" and word[0:-1] in words: 
        #     return pronounce(word[0:-1]) + " s"
        # else: 
        return ["rime not in dictionary â€” need to guess", split[0] + " // " + rime_to_match, 0, onset_tp]

    # print ("total: " + str(total))
    # print ("rime to match: " + str(rime_to_match))
    
    tp = 1 if total == 1 else math.floor(total / math.log(total))
    # print ("tp: " + str(tp))

    if "â€”" in read_onset(split[0])[0]: 
        return [read_onset(split[0])[0], split[0] + " // " + rime_to_match, tp, onset_tp]

    best_rime = ''
    best_rime_count = 0

    for (rime, code) in rime_dict: 
        if rime == rime_to_match:
            if (total - rime_dict[(rime, code)]) <= tp: 
                if rime_dict[(rime, code)] > best_rime_count: 
                    best_rime = code
                    best_rime_count = rime_dict[(rime, code)]
                match_found = True
    
    output += best_rime
    
    if match_found: 
        return [output, split[0] + " // " + rime_to_match, tp, onset_tp]

    if not match_found: 
        return ["rime tolerance principle not met â€” need to memorize exceptions", split[0] + " // " + rime_to_match, tp, onset_tp]

with open ('results.txt', 'w') as f3: 
    correct = 0
    not_enough_info = 0
    for i in range(len(test_words)): 
        f3.write("word: " + str(test_words[i]) + "\n")
        f3.write("split: " + str(pronounce(test_words[i])[1]) + "\n")
        f3.write("onset exceptions allowed: " + str(pronounce(test_words[i])[3]) + "\n")
        f3.write("rime exceptions allowed: " + str(pronounce(test_words[i])[2]) + "\n")
        f3.write("guess: " + str(pronounce(test_words[i])[0]) + "\n")
        f3.write("correct: " + str(test_correct[i]) + "\n")
        if pronounce(test_words[i])[0] == test_correct[i]: 
            correct += 1
            f3.write("right" + "\n")
        elif "â€”" in pronounce(test_words[i])[0]: 
            not_enough_info += 1
            f3.write("not enough information" + "\n")
        else: 
            f3.write("wrong" + "\n")
        f3.write("\n")

    f3.write ("correct: " + str(correct) + "/500" + "\n")
    f3.write ("not enough information: " + str(not_enough_info) + "/500" + "\n")
    f3.write ("wrong: " + str(500 - not_enough_info - correct) + "/500" + "\n")

with open ('rime_dictionary.txt', 'w') as f: 
    f.write (str(rime_dict))